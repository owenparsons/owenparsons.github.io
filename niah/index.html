<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Lessons from Implementing the NIAH Benchmark</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 1000px;
            margin: 0 auto;
            padding: 20px;
            overflow-x: hidden;
            background-color: #D8D9C5;
        }
        h1 {
            text-align: center;
            font-size: 2.5em;
            margin-bottom: 20px;
        }
        .date {
            text-align: center;
            color: #666;
            margin-bottom: 40px;
        }
        .image-container {
            position: relative;
            width: 100%;
            height: 300px;
            overflow: hidden;
        }
        .header-image {
            width: 120%;
            height: 100%;
            object-fit: cover;
            object-position: 50% 75%;
            position: absolute;
            top: 50px;
            left: -10%;
        }
        .content {
            background-color: white;
            padding: 30px;
            position: relative;
            box-shadow: 0 0 10px rgba(0,0,0,0.1);
            margin-top: -100px;
            z-index: 1;
        }
        table {
            width: 70%;
            border-collapse: collapse;
            margin: 20px 0;
            font-size: 16px;
            text-align: right; /* Right-align numeric data */
            font-family: Arial, sans-serif;
        }

        th, td {
            padding: 12px;
            border: 1px solid #ddd;
        }

        th {
            background-color: #f2f2f2;
            font-weight: bold;
            text-align: center; /* Center-align headers */
        }

        td {
            background-color: #fff;
        }

        tr:nth-child(even) {
            background-color: #f9f9f9;
        }

        caption {
            caption-side: top;
            text-align: center;
            font-weight: bold;
            font-size: 18px;
            margin-bottom: 10px;
        }
        a {
              color: #4A4A4A;
              text-decoration: underline;
          }


          a:visited {
              color: #6E6E6E;
              text-decoration: underline;
          }


          a:hover {
              color: #9E9E9E;
              text-decoration: underline;
          }


          a:active {
              color: #3A3A3A;

          }
    </style>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <h1>Lessons from Implementing the 'Needle in a Haystack' Benchmark</h1>
    <p class="date">Owen Parsons - February 5, 2024</p>
    <p class="date"></p>
    <div class="image-container">
        <img src="./figures/header_image.jpg" alt="Header" class="header-image">
    </div>
    <div class="content">

      <h2 id="introduction">Introduction</h2>
      <h3 id="introduction-background">Background</h3>
      <p>With the rapid development and constant evolution of Large Language Models (LLMs), developing robust evaluation frameworks has become increasingly crucial. As these models simultaneously become more sophisticated and also more ubiquitous, understanding their capabilities and limitations through systematic testing is vital.</p>
      <p>I recently had the opportunity to work with the AI Safety Engineering Taskforce (ASET) at <a href="https://www.aracdiaimpact.org/">Arcadia Impact</a>, where our focus was implementing benchmarks for the <a href="https://inspectai.ai/">Inspect AI</a> framework. Inspect AI, developed by the <a href="https://aisafety.org/">AI Safety Institute (AISI)</a>, represents a comprehensive approach to evaluating and understanding the behavior of language models across various dimensions of performance and safety.</p>
      <p>I chose to implement and explore the "Needle in a Haystack" (NIAH) benchmark - a conceptually simple evaluation of long-context retrieval. The benchmark tests an LLM's ability to accurately recall and reference specific information embedded within a larger context, attempting to mimic real-world scenarios where precise information retrieval is crucial. The original implementation of this benchmark was pioneered by Greg Kamradt, who <a href="https://github.com/gkamradt/LLMTest_NeedleInAHaystack">proposed the benchmark</a> and provided an <a href="https://www.youtube.com/watch?v=KwRRuiCCdmc">insightful walkthrough of the concept.</a></p>
      <p>While the basic premise of the benchmark might seem straightforward - and, in theory, less likely to suffer from issues of brittleness - the process of  implementation the benchmark unearthed a few nuances of language models and considerations for designing benchmarks.</p>
      <p>I'll use this post tocapture my main observations and insights from implementing and analysing this benchmark. The aim is to explore not just the technical aspects of this specific implementation, but also broader implications for LLM eval design considerations. I'll discuss some of the challenges I encountered while implementing this benchmarking, highlight some of the limitations of the NIAH benchmark, and suggest some potential improvements.</p>

      

<div id="imageModal" class="modal">
<span class="close" onclick="closeModal()">Ã—</span>
<img class="modal-content" id="fullImage">
</div>

    <script src="scripts.js"></script>
</body>
</html>
